diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 7b034a4..75503d6 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -30,6 +30,7 @@ struct mm_struct;
 #include <linux/init.h>
 #include <linux/err.h>
 #include <linux/irqflags.h>
+#include <linux/lprof.h>
 
 /*
  * We handle most unaligned accesses in hardware.  On the other hand
@@ -497,6 +498,10 @@ struct thread_struct {
 	 * a short time
 	 */
 	unsigned char fpu_counter;
+
+  u64 saved_pmpc[MAX_PMCS];
+  u64 saved_pmes[MAX_PMCS];
+  u64 saved_pmgc;
 };
 
 /*
diff --git a/arch/x86/kernel/cpu/Makefile b/arch/x86/kernel/cpu/Makefile
index 47b56a7..b2f579d 100644
--- a/arch/x86/kernel/cpu/Makefile
+++ b/arch/x86/kernel/cpu/Makefile
@@ -19,6 +19,7 @@ obj-y			+= match.o
 
 obj-$(CONFIG_X86_32)	+= bugs.o
 obj-$(CONFIG_X86_64)	+= bugs_64.o
+obj-$(CONFIG_X86_64)	+= lprof.o
 
 obj-$(CONFIG_CPU_SUP_INTEL)		+= intel.o
 obj-$(CONFIG_CPU_SUP_AMD)		+= amd.o
diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 6abc172..f8df6fb 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1257,6 +1257,9 @@ void cpu_init(void)
 
 	clear_in_cr4(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
 
+  //Allow userland PCM reads
+  set_in_cr4(X86_CR4_PCE);
+
 	/*
 	 * Initialize the per-CPU GDT with the boot GDT,
 	 * and set up the GDT descriptor:
diff --git a/arch/x86/kernel/cpu/lprof.c b/arch/x86/kernel/cpu/lprof.c
new file mode 100644
index 0000000..dbc4eb5
--- /dev/null
+++ b/arch/x86/kernel/cpu/lprof.c
@@ -0,0 +1,489 @@
+/*
+ * Kernel support for LambdaProfiler, a userland, performance
+ * counter-based counter profiler.  Allows users with to configure
+ * performance counters and read them
+ */
+#include <linux/syscalls.h>
+#include <asm/perf_event.h>
+#include <asm/nmi.h>
+#include <asm/msr.h>
+#include <asm/msr-index.h>
+#include <linux/types.h>
+#include <linux/unistd.h>
+#include <linux/cpu.h>
+#include <linux/cpuset.h>
+#include <linux/sched.h>
+#include <linux/lprof.h>
+#include <linux/slab.h>
+#include <asm/uaccess.h>
+
+#define PERFMON_EVENTSEL_INV    (1 << 23)
+#define PERFMON_EVENTSEL_ENABLE (1 << 22)
+#define PERFMON_EVENTSEL_ANY    (1 << 21)
+#define PERFMON_EVENTSEL_INT    (1 << 20)
+#define PERFMON_EVENTSEL_PC     (1 << 19)
+#define PERFMON_EVENTSEL_EDGE   (1 << 18)
+#define PERFMON_EVENTSEL_OS     (1 << 17)
+#define PERFMON_EVENTSEL_USR    (1 << 16)
+
+extern void perf_event_print_debug(void);
+
+struct lprof_cs_pattern {
+    size_t len;
+    unsigned char* pattern;
+};
+
+//Per-task lprof information
+struct lprof_info {
+    struct lprof_stats __user *lp_stats;
+    struct lprof_cs_pattern cs_patterns[MAX_CS];
+};
+
+typedef struct {
+    /* How many tasks are using this counter?*/
+    u32 num_tasks;
+} perf_cntr_t;
+
+static u32 num_perf_counters = 0;
+static perf_cntr_t *perf_counter_info = NULL;
+
+
+static int lp_dfn_cs(unsigned int cs_num, unsigned long long sz,
+		     void __user* ptr) {
+    struct lprof_cs_pattern* pat;
+    if (cs_num >= MAX_CS || sz > MAX_CS_LEN)
+	return -EINVAL;
+
+    if (current->lp_info == NULL) {
+	current->lp_info = (struct lprof_info*)kmalloc(sizeof(struct lprof_info), GFP_KERNEL);
+	memset(current->lp_info, 0, sizeof(struct lprof_info));
+    }
+
+    pat = &current->lp_info->cs_patterns[cs_num];
+    pat->len = sz;
+    pat->pattern = kmalloc(sz, GFP_KERNEL);
+    copy_from_user(pat->pattern, ptr, sz);
+
+    return 0;
+}
+
+static int lp_del_cs(struct task_struct* tsk, unsigned int cs_num) {
+    void* toFree = NULL;
+    if (cs_num >= MAX_CS)
+	return -EINVAL;
+
+    if (!tsk->lp_info)
+	return -EINVAL;
+
+    if (tsk->lp_info->cs_patterns[cs_num].pattern) {
+	toFree = tsk->lp_info->cs_patterns[cs_num].pattern;
+    }
+
+    tsk->lp_info->cs_patterns[cs_num].len = 0;
+    tsk->lp_info->cs_patterns[cs_num].pattern = NULL;
+
+    if (toFree)
+	kfree(toFree);
+
+    return 0;
+}
+
+static int lprof_detect_pattern(size_t len, unsigned char* pat) {
+    size_t i;
+    unsigned char code_buf[MAX_CS_LEN];
+    uint64_t ip = task_pt_regs(current)->ip;
+    unsigned char __user *ins = (unsigned char __user *)ip;
+    unsigned long curr_ip;
+    if (!access_ok(VERIFY_READ, ins, 1))
+	return false;
+
+    get_user(curr_ip, ins);
+
+    for (i=0; i<len; i++) {
+	if (pat[i] == curr_ip) {
+	    //Found an anchor
+	    unsigned char __user *wouldBegin = ins - i;
+	    if (access_ok(VERIFY_READ, wouldBegin, len)) {
+		if (copy_from_user(code_buf, wouldBegin, len) == 0 &&
+		    memcmp(pat, code_buf, len) == 0)
+		    return true;
+	    }
+	}
+    }
+    return false;
+}
+
+static int lprof_detect_cs(void) {
+    int i;
+    struct lprof_info* info = current->lp_info;
+    if (!info)
+	return 0;
+
+    for (i=0; i<MAX_CS; i++) {
+	if (info->cs_patterns[i].len) {
+	    if (lprof_detect_pattern(info->cs_patterns[i].len,
+				     info->cs_patterns[i].pattern))
+		return 1;
+	}
+    }
+    return 0;
+}
+
+static void doLastModify(struct lprof_stats __user* stats, size_t i, u64 p) {
+    unsigned char inprog = 0;
+    u64 user;  // Subtract from user mem
+    //printk("LP Overflow: Modifying user memory at: %p\n", &stats->last[i]);
+    if (get_user(user, &stats->last[i]) == 0) {
+    	user -= p;
+    	put_user(user, &stats->last[i]);
+    }
+
+    get_user(inprog, &stats->rdpmc_inprogress);
+
+    if (inprog || lprof_detect_cs()) {
+    	struct pt_regs *regs = task_pt_regs(current);
+    	size_t ctr = regs->cx; // The counter being read is in %ecx
+    	if (ctr == i) {
+    	    //If an rdpmc operation is in progress on this counter,
+    	    // %rdx and %rax must be zero'ed
+    	    regs->dx = 0;
+    	    regs->ax = 0;
+    	    printk("LProf: Detected atomic counter read on (%lu).  RDX and RAX have been reset.\n", i);
+    	}
+    }
+}
+
+//Process switch
+extern void lprof_switchto(struct task_struct *prev_p,
+			   struct task_struct *next_p) {
+	struct thread_struct *prev = &prev_p->thread;
+	struct thread_struct *next = &next_p->thread;
+	size_t i;
+	unsigned long long g;
+	struct lprof_stats __user *lp_stats = NULL;
+	unsigned char memPresent = false;
+
+	if (prev_p == next_p)
+	    return;
+
+	// Can we safely access user memory?
+	if ((current->state & (TASK_DEAD | TASK_WAKING)) == 0 &&
+	    (current->flags & PF_EXITING) == 0 &&
+	    current->exit_state == 0 &&
+	    current == prev_p &&
+	    current->lp_info &&
+	    current->lp_info->lp_stats &&
+	    access_ok(VERIFY_WRITE,
+		      current->lp_info->lp_stats,
+		      sizeof(struct lprof_stats)) &&
+	    access_ok(VERIFY_READ,
+		      current->lp_info->lp_stats,
+		      sizeof(struct lprof_stats))) {
+	    unsigned long addr = (unsigned long)current->lp_info->lp_stats;
+	    struct vm_area_struct * vma = find_vma(current->mm, addr);
+	    if (vma && 
+		vma->vm_start <= addr &&
+		vma->vm_flags & (VM_READ | VM_WRITE)) {
+		memPresent = true;
+	    } else {
+		printk("VMA Reason: %u, %u, %u, %ld\n",
+		       current->pid,
+		       vma != NULL,
+		       vma->vm_start <= addr,
+		       vma->vm_flags & (VM_READ | VM_WRITE));
+	    }
+	} else {
+	    printk("MP Reason: %u, %u, %u, %u, %u\n", current->pid,
+		   (current->state & (TASK_DEAD | TASK_WAKING)) == 0,
+		   current->exit_state == 0,
+		   current == prev_p,
+		   current->lp_info != NULL);
+	    if (current->lp_info) {
+		printk("MP Reason 2: %u, %u, %ld, %ld\n",
+		       current->pid,
+		       current->lp_info->lp_stats != NULL,
+		       access_ok(VERIFY_WRITE,
+				 current->lp_info->lp_stats,
+				 sizeof(struct lprof_stats)),
+		       access_ok(VERIFY_READ,
+				 current->lp_info->lp_stats,
+				 sizeof(struct lprof_stats)));
+	    }
+	}
+
+	if (memPresent) {
+	    uint64_t cs;
+	    lp_stats = current->lp_info->lp_stats;
+	    if (get_user(cs, &lp_stats->context_switches) == 0) {
+		cs++;
+		put_user(cs, &lp_stats->context_switches);
+	    } else {
+		memPresent = false;
+	    }
+	}
+
+	//Save all perf counter data for prev_p
+	rdmsrl(MSR_CORE_PERF_GLOBAL_CTRL, g);
+	prev->saved_pmgc = g;
+	for (i=0; i<num_perf_counters; i++) {
+	    if (prev_p->perf_counters_inuse & (1 << i)) {
+		u64 c, p;
+		
+		p = native_read_msr(MSR_ARCH_PERFMON_PERFCTR0 + i);
+		c = native_read_msr(MSR_ARCH_PERFMON_EVENTSEL0 + i);
+		
+		if (p >= 0x80000000) {
+		    //printk("LProf: P(%llx) >= 0x80000000.  Reseting and notifying userland.\n", p);
+		    //Cannot write more than 31 bits back
+		    if (lp_stats && memPresent)
+			doLastModify(lp_stats, i, p);
+		    else 
+			printk("Skipping doLastModify ctr (%lu)\n", i);
+		    p = 0;
+		}
+		
+		prev->saved_pmpc[i] = p;
+		prev->saved_pmes[i] = c;
+		
+		//printk("Saved PMC(%lu): %llx\n", i, p);
+	    }
+	}
+	
+	//Restore all perf counter data for next_p
+	g = next->saved_pmgc;
+	wrmsrl(MSR_CORE_PERF_GLOBAL_CTRL, g);
+	for (i=0; i<num_perf_counters; i++) {
+		if (next_p->perf_counters_inuse & (1 << i)) {
+			u64 c, p;
+
+			p = next->saved_pmpc[i];
+			c = next->saved_pmes[i];
+
+			//printk("LProf: Restored %lu with %llx\n", i, p);
+
+			wrmsrl(MSR_ARCH_PERFMON_PERFCTR0 + i, p);
+			wrmsrl(MSR_ARCH_PERFMON_EVENTSEL0 + i, c);
+		} else { //Always write to protect privacy
+			wrmsrl(MSR_ARCH_PERFMON_PERFCTR0 + i, 0);
+			wrmsrl(MSR_ARCH_PERFMON_EVENTSEL0 + i, 0);
+		}
+	}
+}
+
+int init_lprof(void) {
+    union cpuid10_edx edx;
+    union cpuid10_eax eax;
+    unsigned int dummy1, dummy2;
+
+    if (!cpu_has(&boot_cpu_data, X86_FEATURE_ARCH_PERFMON)) {
+	return -ENODEV;
+    }
+
+    //Get CPU info about performance counters and events
+    cpuid(10, &eax.full, &dummy1, &dummy2, &edx.full);
+    num_perf_counters = eax.split.num_counters;
+    if (num_perf_counters > MAX_PMCS) {
+	printk("LProf Warning: found %d counters, max of %d supported. Limiting.\n",
+	       num_perf_counters, MAX_PMCS);
+	num_perf_counters = MAX_PMCS;
+    }
+    perf_counter_info = kmalloc(sizeof(perf_cntr_t) * num_perf_counters, GFP_KERNEL);
+    memset(perf_counter_info, 0, sizeof(perf_cntr_t) * num_perf_counters);
+    printk("LProf init: found %d counters\n", num_perf_counters);
+    return 0;
+}
+
+int lp_stats(struct lprof_stats __user *stats) {
+    if (current->lp_info == NULL) {
+	current->lp_info = (struct lprof_info*)kmalloc(sizeof(struct lprof_info), GFP_KERNEL);
+	memset(current->lp_info, 0, sizeof(struct lprof_info));
+    }
+    current->lp_info->lp_stats = stats;
+    return 0;
+}
+
+int lp_start(unsigned int op, unsigned int counter,
+	     unsigned long long config) {
+    unsigned long long pgc;
+
+    if (counter >= num_perf_counters) {
+	return -ENODEV;
+    }
+
+    if (current->lp_info == NULL) {
+	current->lp_info = (struct lprof_info*)kmalloc(sizeof(struct lprof_info), GFP_KERNEL);
+	memset(current->lp_info, 0, sizeof(struct lprof_info));
+    }
+
+    printk("Opening counter %d.\n", counter);
+
+    //Reserve the request counter if we haven't already
+    if (perf_counter_info[counter].num_tasks == 0) {
+	//Reserve PCMx
+	if (!reserve_perfctr_nmi(MSR_ARCH_PERFMON_PERFCTR0 + counter))
+	    return -EBUSY;
+	
+	//Reserve PerfEvtSelx
+	if (!reserve_evntsel_nmi(MSR_ARCH_PERFMON_EVENTSEL0 + counter)) {
+	    release_perfctr_nmi(MSR_ARCH_PERFMON_PERFCTR0 + counter);
+	    return -EBUSY;
+	}
+    }
+
+    current->perf_counters_inuse |= 1 << counter;
+    perf_counter_info[counter].num_tasks++;
+
+    //Ensure counter is NON-OS and enabled
+    config = 0xFFFFFFFF &
+	(PERFMON_EVENTSEL_ENABLE |  //Require ENABLE
+	 (config & ~(PERFMON_EVENTSEL_PC | PERFMON_EVENTSEL_INT | 
+		     PERFMON_EVENTSEL_ANY)));  //Disallow PC, INT and ANY for security reasons
+                      
+
+    //Reset counter
+    wrmsr(MSR_ARCH_PERFMON_PERFCTR0 + counter, 0, 0);
+    //Configure Counter
+    wrmsr(MSR_ARCH_PERFMON_EVENTSEL0 + counter, config, 0);
+    //Clear a possible counter overflow
+    wrmsrl(MSR_CORE_PERF_GLOBAL_OVF_CTRL, 1 << counter);
+
+    rdmsrl(MSR_CORE_PERF_GLOBAL_CTRL, pgc);
+    pgc |= 1 << counter;
+    wrmsrl(MSR_CORE_PERF_GLOBAL_CTRL, pgc);
+
+    return 0;
+}
+
+int lp_stop(struct task_struct* tsk,
+	    unsigned int op, unsigned int counter,
+	    unsigned long long config) {
+    unsigned long long pgc;
+    size_t i;
+
+    if (!(tsk->perf_counters_inuse & (1 << counter))) {
+	return -EINVAL;
+    }
+
+    //printk("Closing counter %d.\n", counter);
+
+    tsk->perf_counters_inuse &= ~(1 << counter);
+
+    //Disable counter
+    config &= ~ARCH_PERFMON_EVENTSEL_ENABLE;
+    wrmsrl(MSR_ARCH_PERFMON_EVENTSEL0 + counter, config);
+
+    rdmsrl(MSR_CORE_PERF_GLOBAL_CTRL, pgc);
+    pgc &= ~(1 << counter);
+    wrmsrl(MSR_CORE_PERF_GLOBAL_CTRL, pgc);
+
+    if (tsk->perf_counters_inuse == 0 && 
+	tsk->lp_info) {
+	void *ptr;
+
+	//Free up patterns
+	for (i=0; i<MAX_CS; i++) {
+	    if (tsk->lp_info->cs_patterns[i].pattern)
+		lp_del_cs(tsk, i);
+	}
+
+	ptr = tsk->lp_info;
+	tsk->lp_info = NULL;
+	kfree(ptr);
+    }
+
+    perf_counter_info[counter].num_tasks--;
+    if (perf_counter_info[counter].num_tasks == 0) {
+	//Release resources back to kernel
+	release_perfctr_nmi(MSR_ARCH_PERFMON_PERFCTR0 + counter);
+	release_evntsel_nmi(MSR_ARCH_PERFMON_EVENTSEL0 + counter);
+	//printk("Releasing counter %d to kernel\n", counter);
+    }
+    return 0;
+}
+
+static int lp_debug(unsigned int op, unsigned int counter,
+		    unsigned long long config) {
+    u32 i;
+    printk("----- LProf Debug -----\n");
+    perf_event_print_debug();
+    printk("In use:\n");
+    for (i=0; i<num_perf_counters; i++) {
+	printk("\tCounter %d: %d tasks\n", i, perf_counter_info[i].num_tasks);
+    }
+
+    return 0;
+}
+
+/**
+ * sys_perf_counter_open - open a performance counter, associate it to a task/cpu
+ *
+ * @op:                 Bitmap of operations to conduct
+ * @counter:		The number of the counter to use
+ * @config:             The value to use in PERFEVTSELx
+ * @stats:              Userland memory location to store scheduling
+ *                          stats or NULL
+ */
+SYSCALL_DEFINE4(lprof_config,
+		unsigned int, op,
+		unsigned int, counter,
+		unsigned long long, config,
+		void __user *, ptr)
+{
+    int rc;
+    //Do we need to initialize?
+    if (!perf_counter_info) {
+	int rc = init_lprof();
+	if (rc < 0)
+	    return rc;
+    }
+
+    //What operation is requested?
+    switch (op & LPROF_OP_MASK) {
+    case LPROF_STOP:
+printk("Closing counter %d.\n", counter);
+	return lp_stop(current, op, counter, config);
+    case LPROF_START:
+	if ((rc = lp_start(op, counter, config)))
+	    return rc;
+    case LPROF_STATS:
+	return lp_stats((struct lprof_stats __user*)ptr);
+    case LPROF_DBG:
+	return lp_debug(op, counter, config);
+    case LPROF_DFN_CS:
+	return lp_dfn_cs(counter, config, ptr);
+    case LPROF_DEL_CS:
+	return lp_del_cs(current, counter);
+    default:
+	return -EINVAL;
+    }
+}
+
+void exit_lprof(struct task_struct* tsk) {
+    size_t i;
+
+    //Close open counters
+    if (tsk->perf_counters_inuse && perf_counter_info) {
+	u32 i;
+	for (i=0; i<num_perf_counters; i++) {
+	    if (tsk->perf_counters_inuse & (1 << i))
+		lp_stop(tsk, 0, i, 0);
+	}
+    }
+
+    tsk->perf_counters_inuse = 0;
+
+    //Free CS patterns
+    if (tsk->lp_info) {
+	for (i=0; i<MAX_CS; i++) {
+	    if (tsk->lp_info->cs_patterns[i].pattern)
+		lp_del_cs(tsk, i);
+	}
+    }
+    
+    //Free our process information
+    if (tsk->lp_info) {
+	void* ptr = tsk->lp_info;
+	tsk->lp_info = NULL;
+	kfree(ptr);
+    }
+}
diff --git a/arch/x86/syscalls/syscall_64.tbl b/arch/x86/syscalls/syscall_64.tbl
index 38ae65d..3653bfe 100644
--- a/arch/x86/syscalls/syscall_64.tbl
+++ b/arch/x86/syscalls/syscall_64.tbl
@@ -320,6 +320,7 @@
 311	64	process_vm_writev	sys_process_vm_writev
 312	common	kcmp			sys_kcmp
 313	common	finit_module		sys_finit_module
+400	common	lprof_config		sys_lprof_config
 
 #
 # x32-specific system call numbers start at 512 to avoid cache impact
diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index b0ed422..8968574 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -213,6 +213,8 @@ extern struct task_group root_task_group;
 		[PIDTYPE_SID]  = INIT_PID_LINK(PIDTYPE_SID),		\
 	},								\
 	.thread_group	= LIST_HEAD_INIT(tsk.thread_group),		\
+	.perf_counters_inuse = 0,					\
+	.lp_info = NULL,						\
 	INIT_IDS							\
 	INIT_PERF_EVENTS(tsk)						\
 	INIT_TRACE_IRQFLAGS						\
diff --git a/include/linux/lprof.h b/include/linux/lprof.h
new file mode 100644
index 0000000..10a162e
--- /dev/null
+++ b/include/linux/lprof.h
@@ -0,0 +1,75 @@
+/*
+ * LambdaProfiler Fine-Grandularaity Userland Performance Counter support
+ *
+ *    Copyright (C) 2010, John Demme, Columbia University
+ *
+ */
+#ifndef _LINUX_LPROF_H
+#define _LINUX_LPROF_H
+
+#ifndef __KERNEL__
+#include <unistd.h>
+#include <stdint.h>
+#endif
+
+#define MAX_PMCS   8
+#define MAX_CS     8
+#define MAX_CS_LEN 256
+
+#define LPROF_OP_MASK   0x7
+
+#define LPROF_STOP      (0)  //Start or stop profiling?
+#define LPROF_START     (1)  //Start or stop profiling?
+#define LPROF_DBG       (2)  //Print debug info to kernel log
+#define LPROF_STATS     (3)  //Inform kernel about stats location
+#define LPROF_DFN_CS    (4)  //Define instruction pattern of LP
+			     //critical section
+#define LPROF_DEL_CS    (5)  //Delete LP critical section
+
+#ifndef _LPROF_H
+struct lprof_stats {
+    unsigned char rdpmc_inprogress; /* Offset: 0 bytes */
+    uint64_t context_switches;	    /* Offset: 8 byte */
+    int64_t last[MAX_PMCS];	    /* Offsets: [0]: 16
+				                [1]: 24 
+						[2]: 32 ...*/
+};
+#endif
+
+#ifndef __KERNEL__
+
+#ifndef _GNU_SOURCE
+#define _GNU_SOURCE
+#endif
+
+#define __NR_lprof_config 299
+
+static inline int
+sys_lprof_config(unsigned int op, unsigned int counter,
+		 unsigned long long config, void* ptr)
+{
+    return syscall(__NR_lprof_config, op, counter, config, ptr);
+}
+
+#define DECLARE_ARGS(val, low, high)	unsigned low, high
+#define EAX_EDX_VAL(val, low, high)	((low) | ((unsigned long long)(high) << 32))
+#define EAX_EDX_ARGS(val, low, high)	"a" (low), "d" (high)
+#define EAX_EDX_RET(val, low, high)	"=a" (low), "=d" (high)
+static inline unsigned long long native_read_pmc(int counter)
+{
+	DECLARE_ARGS(val, low, high);
+
+	asm volatile("rdpmc" : EAX_EDX_RET(val, low, high) : "c" (counter));
+	return EAX_EDX_VAL(val, low, high);
+}
+#endif //__KERNEL__
+
+#ifdef __KERNEL__
+
+struct task_struct;
+extern void exit_lprof(struct task_struct*);
+extern void lprof_tick(void);
+extern void lprof_switchto(struct task_struct *prev_p, struct task_struct *next_p);
+
+#endif //__KERNEL__
+#endif //_LINUX_LPROF_H
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 53f97eb..3336195 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1031,6 +1031,7 @@ struct sched_rt_entity {
 
 
 struct rcu_node;
+struct lprof_info;
 
 enum perf_event_task_context {
 	perf_invalid_context = -1,
@@ -1454,6 +1455,12 @@ struct task_struct {
 	unsigned int	sequential_io;
 	unsigned int	sequential_io_avg;
 #endif
+
+       /* LProf accounting info */
+
+       /* Bit map of the performance counts in use */
+       unsigned long perf_counters_inuse;
+       struct lprof_info *lp_info;
 };
 
 /* Future-safe accessor for struct task_struct's cpus_allowed. */
diff --git a/kernel/exit.c b/kernel/exit.c
index a949819..e8fb063 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -53,6 +53,7 @@
 #include <linux/oom.h>
 #include <linux/writeback.h>
 #include <linux/shm.h>
+#include <linux/lprof.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -698,6 +699,7 @@ void do_exit(long code)
 	struct task_struct *tsk = current;
 	int group_dead;
 
+	exit_lprof(tsk);
 	profile_task_exit(tsk);
 
 	WARN_ON(blk_needs_flush_plug(tsk));
diff --git a/kernel/fork.c b/kernel/fork.c
index dfa736c..f778c21 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1298,6 +1298,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->lockdep_recursion = 0;
 #endif
 
+	p->perf_counters_inuse = 0;
+	p->lp_info = NULL;
+
 #ifdef CONFIG_DEBUG_MUTEXES
 	p->blocked_on = NULL; /* not blocked yet */
 #endif
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index a88f4a4..9d04738 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -73,6 +73,7 @@
 #include <linux/init_task.h>
 #include <linux/binfmts.h>
 #include <linux/context_tracking.h>
+#include <linux/lprof.h>
 
 #include <asm/switch_to.h>
 #include <asm/tlb.h>
@@ -2085,6 +2086,10 @@ context_switch(struct rq *rq, struct task_struct *prev,
 {
 	struct mm_struct *mm, *oldmm;
 
+	//Do the lprof context switch here.  Must happen before MM change
+	if (prev->perf_counters_inuse || next->perf_counters_inuse)
+		lprof_switchto(prev, next);
+
 	prepare_task_switch(rq, prev, next);
 
 	mm = next->mm;
